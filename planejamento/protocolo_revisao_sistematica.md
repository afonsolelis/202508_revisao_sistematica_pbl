# Systematic Review Protocol: Assessment Challenges in Project-Based Learning

## 1. Research Objective

To systematically map the scientific literature on assessment challenges in Project-Based Learning (PBL), identifying methodological difficulties, instruments, technologies, and persistent gaps in evaluating student learning in PBL contexts.

## 2. Research Questions

### Primary Research Questions:
1. **RQ1**: What are the main methodological challenges in assessing learning in PBL contexts?
2. **RQ2**: What instruments and technologies are being used to overcome these challenges?
3. **RQ3**: How can technology support more objective and scalable assessment in PBL?
4. **RQ4**: What gaps persist in assessing processual and collaborative competencies?

### Secondary Research Questions:
1. How do assessment challenges vary across different educational levels (K-12, higher education, professional training)?
2. What are the most effective strategies for assessing transversal competencies in PBL?
3. How do cultural and institutional factors influence PBL assessment practices?

## 3. Conceptual Framework

### Theoretical Foundation
- Constructivist learning theory
- Assessment for learning (formative assessment)
- Competency-based education
- Collaborative learning theory

### Key Concepts
- **Project-Based Learning (PBL)**: Student-centered pedagogy where learners engage with complex, real-world problems through extended projects
- **Assessment Challenges**: Methodological, practical, and conceptual difficulties in evaluating student learning in PBL contexts
- **Processual Assessment**: Evaluation of learning throughout the project development process rather than just the final product
- **Collaborative Competencies**: Skills developed through teamwork, including communication, cooperation, and collective problem-solving
- **Transversal Skills**: Cross-cutting competencies such as critical thinking, creativity, and adaptability

## 4. Inclusion and Exclusion Criteria

### Inclusion Criteria (IC)

#### IC1: Focus on PBL Assessment
Articles that specifically address:
- Assessment methods in Project-Based Learning
- Evaluation instruments for PBL contexts
- Rubrics and criteria for project evaluation
- Feedback mechanisms in PBL

#### IC2: Assessment Challenges
Articles that discuss:
- Methodological difficulties in PBL assessment
- Problems with evaluating collaborative work
- Challenges in measuring processual learning
- Issues with assessing transversal competencies

#### IC3: Instruments and Technologies
Articles that present or evaluate:
- Assessment instruments (rubrics, scales, protocols)
- Technological tools for PBL assessment
- Automated or semi-automated assessment systems
- Learning analytics for project evaluation

#### IC4: Educational Context
Articles set in:
- Formal educational environments (schools, universities)
- Professional training contexts
- STEM education
- Interdisciplinary learning contexts

#### IC5: Empirical Research
Articles that include:
- Empirical studies with student participants
- Validation of assessment instruments
- Comparative studies of assessment methods
- Case studies of PBL implementation

### Exclusion Criteria (EC)

#### EC1: Non-PBL Contexts
Exclude articles focused on:
- Traditional lecture-based instruction
- Pure problem-based learning (medical)
- General project work without pedagogical framework
- Corporate project management

#### EC2: Non-Assessment Focus
Exclude articles that:
- Focus only on PBL implementation without assessment
- Address only content delivery in PBL
- Discuss technological platforms without assessment features
- Present only theoretical frameworks without empirical validation

#### EC3: Insufficient Academic Rigor
Exclude articles that:
- Lack empirical validation
- Are purely theoretical/conceptual
- Are conference abstracts or posters
- Are literature reviews or meta-analyses

#### EC4: Outdated Research
Exclude articles published before 2015 to ensure:
- Contemporary assessment challenges
- Current technological solutions
- Recent educational policy contexts

#### EC5: Irrelevant Domains
Exclude articles focused on:
- Industrial or corporate training without educational context
- Medical education (problem-based learning specific)
- Pure software engineering without pedagogical focus
- Vocational training without transferable insights

## 5. Search Strategy

### Database Selection
**Primary Database**: Web of Science
Justification:
- High-quality, peer-reviewed publications
- Comprehensive coverage of education and technology journals
- Robust citation metrics
- Alignment with systematic review standards

### Search Strings
See separate document: `strings_busca_alinhadas.md`

### Time Frame
January 2015 - December 2025

### Languages
English, Portuguese, Spanish

### Document Types
- Journal articles
- Conference papers
- Book chapters

## 6. Study Selection Process

### Phase 1: Identification
- Import all records from database searches
- Remove duplicates using reference management software

### Phase 2: Screening
- Review titles and abstracts against inclusion criteria
- Exclude clearly irrelevant studies
- Document reasons for exclusion

### Phase 3: Eligibility
- Retrieve full texts of potentially relevant articles
- Apply inclusion/exclusion criteria to full texts
- Resolve disagreements through discussion or third reviewer

### Phase 4: Inclusion
- Finalize list of included studies
- Extract data using standardized forms
- Begin quality assessment

## 7. Data Extraction

### Data to be Extracted
1. **Study Characteristics**:
   - Authors, year, journal
   - Study design and methodology
   - Educational context and level
   - Sample size and characteristics

2. **PBL Implementation**:
   - Subject area/discipline
   - Project duration and complexity
   - Group size and collaboration structure

3. **Assessment Approaches**:
   - Methods used (rubrics, peer assessment, etc.)
   - Instruments employed
   - Technologies utilized

4. **Challenges Identified**:
   - Explicitly mentioned difficulties
   - Implicit issues revealed through methodology
   - Stakeholder perspectives (students, teachers, administrators)

5. **Solutions Proposed**:
   - Instruments developed
   - Technologies implemented
   - Methodological innovations

6. **Outcomes and Findings**:
   - Effectiveness of assessment approaches
   - Impact on student learning
   - Teacher and student perceptions

## 8. Quality Assessment

### Risk of Bias Assessment
Using appropriate tools based on study design:
- **Experimental/quasi-experimental**: Revised Cochrane Risk of Bias Tool
- **Observational studies**: ROBINS-I tool
- **Qualitative studies**: CASP checklist

### Methodological Rigor
Assess:
- Sample size justification
- Data collection procedures
- Analysis methods
- Reporting transparency

## 9. Data Synthesis

### Thematic Analysis
- Identify common themes related to assessment challenges
- Categorize instruments and technologies by function
- Map solutions to specific challenges

### Narrative Synthesis
- Describe patterns in the literature
- Explain relationships between concepts
- Highlight contradictions and gaps

### Descriptive Statistics
- Characterize the body of evidence
- Analyze temporal trends
- Map geographical distribution

## 10. Reporting Standards

This systematic review will be reported according to the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines.

## 11. Ethical Considerations

As this is a literature review, no primary data collection from human subjects is involved. All data will be handled in accordance with academic integrity standards.

## 12. Dissemination Plan

Results will be:
- Written up in academic manuscript format
- Submitted to peer-reviewed journal in education or educational technology
- Presented at relevant conferences
- Shared with educational practitioners through workshops or webinars