# 3. Results

## 3.1 Study Selection Process

The initial search identified 811 articles through the Web of Science database. After removing duplicates, 632 unique articles were screened based on titles and abstracts. Of these, 289 articles were considered potentially relevant and underwent full-text evaluation. Following the application of inclusion and exclusion criteria, 179 articles were included in the final systematic review.

The selection process followed the PRISMA framework:

```
Records identified through database searching (n=811)
│
├── Duplicates removed (n=179)
│
├── Records screened (n=632)
│
├── Records excluded (n=343)
│   ├── Non-PBL contexts (n=125)
│   ├── No assessment focus (n=89)
│   ├── Theoretical only (n=72)
│   ├── Conference abstracts (n=34)
│   ├── Wrong time period (n=23)
│
├── Full-text articles assessed for eligibility (n=289)
│
├── Full-text articles excluded (n=110)
│   ├── Non-educational context (n=42)
│   ├── No empirical validation (n=31)
│   ├── Wrong publication type (n=23)
│   ├── Language restrictions (n=14)
│
└── Studies included in qualitative synthesis (n=179)
```

## 3.2 Characterization of Included Studies

### 3.2.1 Temporal Distribution
The included studies showed consistent publication activity from 2015 to 2025, with 54.2% (n=97) published in the most recent years (2020-2025). Peak publication years were 2016 (n=21), 2017 (n=19), and 2019 (n=19), indicating sustained research interest in PBL assessment.

### 3.2.2 Geographical Distribution
Research was concentrated in international venues (36.0%) and the USA (27.2%), with limited representation from developing regions. This distribution suggests a need for more diverse, cross-cultural approaches to PBL assessment.

### 3.2.3 Publication Venues
The majority of articles were published in specialized education journals, with prominent venues including IEEE Transactions on Education, International Journal of Engineering Education, and the Interdisciplinary Journal of Problem-Based Learning.

## 3.3 Technological Categorization

The technological focus of the included studies revealed significant patterns:

- **Programming Tools**: 67.0% (n=120) of articles focused on programming-related assessment tools
- **General PBL**: 14.5% (n=26) addressed broader PBL assessment frameworks
- **General Technology**: 14.0% (n=25) covered general educational technologies
- **Machine Learning/AI**: 6.7% (n=12) explored AI-based assessment approaches
- **Assessment Tools**: 3.4% (n=6) presented specific assessment instruments
- **Software Architecture**: 3.4% (n=6) discussed architectural approaches to educational systems

Notably, critical emerging technologies showed minimal representation:
- Digital Twins: 0.6% (n=1)
- DevOps/CI-CD: 0.6% (n=1)
- Automated Assessment: 0.6% (n=1)

## 3.4 Methodological Approaches

The methodological approaches to PBL assessment revealed a predominance of traditional methods:

- **General Assessment**: 58.1% (n=104) focused on general assessment methods
- **Formative Assessment**: 21.2% (n=38) addressed ongoing evaluation during PBL processes
- **Summative Assessment**: 11.7% (n=21) concentrated on final product evaluation
- **Peer Assessment**: 7.8% (n=14) explored collaborative evaluation methods
- **Self Assessment**: 7.3% (n=13) examined student reflection mechanisms
- **Automated Assessment**: 3.4% (n=6) investigated technology-driven evaluation

Advanced approaches such as learning analytics were underrepresented (0.6%, n=1).

## 3.5 Key Assessment Challenges Identified

### 3.5.1 Processual Assessment
A major challenge identified was the difficulty in assessing learning throughout the project development process rather than just the final product. Extended PBL projects (2-4 months) involve multiple iterations where student competencies evolve non-linearly, making point-in-time assessments inadequate for capturing learning progression.

### 3.5.2 Collaborative Competencies
Evaluating individual contributions within team-based projects emerged as a persistent challenge. Studies highlighted difficulties in:
- Identifying and measuring individual contributions in interdependent work
- Assessing team dynamics and interpersonal skills
- Evaluating rotating roles and responsibilities throughout the project

### 3.5.3 Transversal Skills
The assessment of transversal competencies such as critical thinking, creativity, communication, and collaboration proved challenging due to:
- Subjectivity in evaluating qualitative skills
- Intangibility of certain competencies in technical contexts
- Contextual variation in skill manifestation

### 3.5.4 Scalability and Objectivity
Instructors working with large classes faced challenges in maintaining assessment quality while scaling their evaluation efforts:
- Time constraints for detailed individual assessment
- Consistency maintenance across multiple evaluators
- Balancing personalization with efficiency requirements

## 3.6 Instruments and Technologies for Assessment

### 3.6.1 Rubrics and Structured Instruments
Structured rubrics emerged as the most commonly used assessment instruments, with variations including:
- Holistic rubrics for overall project evaluation
- Analytical rubrics for detailed component assessment
- Developmental rubrics tracking competency progression

### 3.6.2 Digital Assessment Platforms
Specialized digital platforms demonstrated potential for addressing multiple assessment challenges:
- Project management systems integrating planning, execution, and evaluation
- Digital portfolios documenting evolution over time
- Structured feedback systems increasing frequency and consistency

### 3.6.3 Learning Analytics
Data-driven approaches began to show promise:
- Engagement monitoring through participation metrics
- Performance prediction for early intervention
- Process visualization for instructors and students

### 3.6.4 Technical Metrics
Domain-specific metrics proved effective in technical PBL contexts:
- Code quality metrics for programming projects
- Version control analysis for collaboration assessment
- Activity tracking for progress monitoring

## 3.7 Critical Research Gaps

### 3.7.1 Digital Twins in Education
Only one article (0.6%) explicitly addressed Digital Twins in educational contexts, representing a significant gap given the potential of this technology for creating real-time replicas of educational processes.

### 3.7.2 Real-time Instructor Support
Minimal research was found on technologies supporting instructors during active PBL processes, despite the clear need for real-time decision-making support.

### 3.7.3 Scalable Assessment Architectures
Limited research explored architecture-based approaches to educational assessment systems, particularly those following established software architecture standards such as ISO 42010.

### 3.7.4 Industry Integration
Insufficient research examined the integration of industry perspectives and practices into academic assessment tools, despite the professional orientation of many PBL programs.